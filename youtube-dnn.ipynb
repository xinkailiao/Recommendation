{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1872300,"sourceType":"datasetVersion","datasetId":1114664}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:47:08.597514Z","iopub.execute_input":"2025-09-18T08:47:08.597806Z","iopub.status.idle":"2025-09-18T08:47:09.000048Z","shell.execute_reply.started":"2025-09-18T08:47:08.597785Z","shell.execute_reply":"2025-09-18T08:47:08.999112Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/movielens-1m-dataset/users.dat\n/kaggle/input/movielens-1m-dataset/ratings.dat\n/kaggle/input/movielens-1m-dataset/README\n/kaggle/input/movielens-1m-dataset/movies.dat\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:16:46.195247Z","iopub.execute_input":"2025-09-14T07:16:46.195582Z","iopub.status.idle":"2025-09-14T07:16:46.232673Z","shell.execute_reply.started":"2025-09-14T07:16:46.195556Z","shell.execute_reply":"2025-09-14T07:16:46.231725Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  gender  user_id  occupation zip-code\n1      F        1          10    48067\n2      M       56          16    70072\n3      M       25          15    55117\n4      M       45           7    02460\n5      M       25          20    55455","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>user_id</th>\n      <th>occupation</th>\n      <th>zip-code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>56</td>\n      <td>16</td>\n      <td>70072</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>25</td>\n      <td>15</td>\n      <td>55117</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>45</td>\n      <td>7</td>\n      <td>02460</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>M</td>\n      <td>25</td>\n      <td>20</td>\n      <td>55455</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df_item = pd.read_csv('/kaggle/input/movielens-1m-dataset/ratings.dat', sep='::', engine='python',\n                 names=['movie_id', 'title', 'genres'])\ndf_item.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:18:04.738980Z","iopub.execute_input":"2025-09-14T07:18:04.739302Z","iopub.status.idle":"2025-09-14T07:18:10.016879Z","shell.execute_reply.started":"2025-09-14T07:18:04.739279Z","shell.execute_reply":"2025-09-14T07:18:10.015947Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   movie_id  title     genres\n1      1193      5  978300760\n1       661      3  978302109\n1       914      3  978301968\n1      3408      4  978300275\n1      2355      5  978824291","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"users_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/users.dat',\n                       header=None, \n                       sep='::', \n                       names=['UserID','Gender','Age','Occupation','Zip-code'], \n                       engine='python',\n                       encoding='latin-1')\n\n\nmovies_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/movies.dat',\n                        header=None,\n                        sep='::',\n                        names=['MovieID', 'Title', 'Genre'], \n                        engine='python',\n                        encoding='latin-1')\n\nratings_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/ratings.dat',\n                         header=None,\n                         sep='::',\n                         names=['UserID','MovieID','Rating','Timestamp'], \n                         engine='python',\n                         encoding='latin-1')\n#使用engine是因为sep为：:,如果是单字符就无所谓","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:56:20.949607Z","iopub.execute_input":"2025-09-18T08:56:20.949922Z","iopub.status.idle":"2025-09-18T08:56:26.500018Z","shell.execute_reply.started":"2025-09-18T08:56:20.949899Z","shell.execute_reply":"2025-09-18T08:56:26.498749Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = pd.merge(pd.merge(ratings_df, users_df), movies_df) #自动合并，不足的行默认删除\ndata.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:18:01.358849Z","iopub.execute_input":"2025-09-18T09:18:01.359226Z","iopub.status.idle":"2025-09-18T09:18:01.701320Z","shell.execute_reply.started":"2025-09-18T09:18:01.359200Z","shell.execute_reply":"2025-09-18T09:18:01.699768Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   UserID  MovieID  Rating  Timestamp Gender  Age  Occupation Zip-code  \\\n0       1     1193       5  978300760      F    1          10    48067   \n1       1      661       3  978302109      F    1          10    48067   \n2       1      914       3  978301968      F    1          10    48067   \n3       1     3408       4  978300275      F    1          10    48067   \n4       1     2355       5  978824291      F    1          10    48067   \n\n                                    Title                         Genre  \n0  One Flew Over the Cuckoo's Nest (1975)                         Drama  \n1        James and the Giant Peach (1996)  Animation|Children's|Musical  \n2                     My Fair Lady (1964)               Musical|Romance  \n3                  Erin Brockovich (2000)                         Drama  \n4                    Bug's Life, A (1998)   Animation|Children's|Comedy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserID</th>\n      <th>MovieID</th>\n      <th>Rating</th>\n      <th>Timestamp</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Occupation</th>\n      <th>Zip-code</th>\n      <th>Title</th>\n      <th>Genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n      <td>James and the Giant Peach (1996)</td>\n      <td>Animation|Children's|Musical</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n      <td>My Fair Lady (1964)</td>\n      <td>Musical|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n      <td>Erin Brockovich (2000)</td>\n      <td>Drama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n      <td>F</td>\n      <td>1</td>\n      <td>10</td>\n      <td>48067</td>\n      <td>Bug's Life, A (1998)</td>\n      <td>Animation|Children's|Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"#对离散向量和连续向量进行分类\nuser_col, item_col = \"UserID\", \"MovieID\"\nsparse_features = ['UserID', 'MovieID', 'Gender', 'Age', 'Occupation', 'Zip-code', \"Genre\"]\ndense_features = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:18:04.115323Z","iopub.execute_input":"2025-09-18T09:18:04.115735Z","iopub.status.idle":"2025-09-18T09:18:04.122115Z","shell.execute_reply.started":"2025-09-18T09:18:04.115710Z","shell.execute_reply":"2025-09-18T09:18:04.120945Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"save_dir = '/kaggle/working//ml-1m/saved/'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:18:06.660380Z","iopub.execute_input":"2025-09-18T09:18:06.660771Z","iopub.status.idle":"2025-09-18T09:18:06.665797Z","shell.execute_reply.started":"2025-09-18T09:18:06.660747Z","shell.execute_reply":"2025-09-18T09:18:06.664568Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# 对SparseFeature进行LabelEncoding\nfrom sklearn.preprocessing import LabelEncoder\nprint(data[sparse_features].head())\nfeature_max_idx = {}\nfor feature in sparse_features:\n    lbe = LabelEncoder()\n    data[feature] = lbe.fit_transform(data[feature]) + 1  # 删除 0 值\n    feature_max_idx[feature] = data[feature].max() + 1  # 多出来的 1 应该是为了 unseen 类别做保留，比如新商品、新用户\n    if feature == user_col:  # lbe.classes_的值会随着 lbe.fit_transform 处理的数据而变化，有对应关系；leb.classes_是类属性\n        user_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}  #encode user id: raw user id\n    if feature == item_col:\n        item_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(lbe.classes_)}  #encode item id: raw item id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:18:06.918016Z","iopub.execute_input":"2025-09-18T09:18:06.918491Z","iopub.status.idle":"2025-09-18T09:18:07.767247Z","shell.execute_reply.started":"2025-09-18T09:18:06.918465Z","shell.execute_reply":"2025-09-18T09:18:07.765741Z"}},"outputs":[{"name":"stdout","text":"   UserID  MovieID Gender  Age  Occupation Zip-code  \\\n0       1     1193      F    1          10    48067   \n1       1      661      F    1          10    48067   \n2       1      914      F    1          10    48067   \n3       1     3408      F    1          10    48067   \n4       1     2355      F    1          10    48067   \n\n                          Genre  \n0                         Drama  \n1  Animation|Children's|Musical  \n2               Musical|Romance  \n3                         Drama  \n4   Animation|Children's|Comedy  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"np.save(save_dir+\"raw_id_maps.npy\", (user_map, item_map))  # evaluation时会用到\nprint('LabelEncoding后：')\nprint(data[sparse_features].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:18:07.768847Z","iopub.execute_input":"2025-09-18T09:18:07.769150Z","iopub.status.idle":"2025-09-18T09:18:07.824065Z","shell.execute_reply.started":"2025-09-18T09:18:07.769105Z","shell.execute_reply":"2025-09-18T09:18:07.822745Z"}},"outputs":[{"name":"stdout","text":"LabelEncoding后：\n   UserID  MovieID  Gender  Age  Occupation  Zip-code  Genre\n0       1     1105       1    1          11      1589    240\n1       1      640       1    1          11      1589    153\n2       1      854       1    1          11      1589    283\n3       1     3178       1    1          11      1589    240\n4       1     2163       1    1          11      1589    146\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 定义两个塔对应哪些特征\nuser_cols = [\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]\nitem_cols = ['MovieID', \"Genre\"]\n\n# 从data中取出相应的数据\nuser_profile = data[user_cols].drop_duplicates('UserID')  # 去重\nitem_profile = data[item_cols].drop_duplicates('MovieID')\nprint(user_profile.head())\nprint(item_profile.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:22:49.319447Z","iopub.execute_input":"2025-09-18T09:22:49.319754Z","iopub.status.idle":"2025-09-18T09:22:49.377452Z","shell.execute_reply.started":"2025-09-18T09:22:49.319733Z","shell.execute_reply":"2025-09-18T09:22:49.376323Z"}},"outputs":[{"name":"stdout","text":"     UserID  Gender  Age  Occupation  Zip-code\n0         1       1    1          11      1589\n53        2       2    7          17      2249\n182       3       2    3          16      1864\n233       4       2    5           8       141\n254       5       2    3          21      1939\n   MovieID  Genre\n0     1105    240\n1      640    153\n2      854    283\n3     3178    240\n4     2163    146\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def generate_seq_feature_match(data,\n                               user_col,\n                               item_col,\n                               time_col,\n                               item_attribute_cols=[],\n                               sample_method=0,\n                               mode=0,\n                               neg_ratio=0,\n                               min_item=0):\n    \"\"\"generate sequence feature and negative sample for match.\n\n    Args:\n        data (pd.DataFrame): the raw data.\n        user_col (str): the col name of user_id \n        item_col (str): the col name of item_id \n        time_col (str): the col name of timestamp\n        item_attribute_cols (list[str], optional): the other attribute cols of item which you want to generate sequence feature. Defaults to `[]`.\n        sample_method (int, optional): the negative sample method `{\n            0: \"random sampling\", \n            1: \"popularity sampling method used in word2vec\", \n            2: \"popularity sampling method by `log(count+1)+1e-6`\",\n            3: \"tencent RALM sampling\"}`. \n            Defaults to 0.\n        mode (int, optional): the training mode, `{0:point-wise, 1:pair-wise, 2:list-wise}`. Defaults to 0.\n        neg_ratio (int, optional): negative sample ratio, >= 1. Defaults to 0.\n        min_item (int, optional): the min item each user must have. Defaults to 0.\n\n    Returns:\n        pd.DataFrame: split train and test data with sequence features.\n    \"\"\"\n    if mode == 2:  # list wise learning\n        assert neg_ratio > 0, 'neg_ratio must be greater than 0 when list-wise learning'\n    elif mode == 1:  # pair wise learning\n        neg_ratio = 1\n    print(\"preprocess data\")\n    data.sort_values(time_col, inplace=True)  #sort by time from old to new\n    train_set, test_set = [], []\n    n_cold_user = 0\n\n    items_cnt = Counter(data[item_col].tolist()) #计算物品出现的次数即热门程度\n    items_cnt_order = OrderedDict(sorted((items_cnt.items()), key=lambda x: x[1], reverse=True))  #item_id:item count\n    neg_list = negative_sample(items_cnt_order, ratio=data.shape[0] * neg_ratio, method_id=sample_method)\n    neg_idx = 0\n    for uid, hist in tqdm.tqdm(data.groupby(user_col), desc='generate sequence features'):\n        pos_list = hist[item_col].tolist()\n        if len(pos_list) < min_item:  #drop this user when his pos items < min_item\n            n_cold_user += 1\n            continue\n\n        for i in range(1, len(pos_list)):\n            hist_item = pos_list[:i] #时间序列窗口\n            sample = [uid, pos_list[i], hist_item, len(hist_item)]\n            if len(item_attribute_cols) > 0:\n                for attr_col in item_attribute_cols:  #the history of item attribute features\n                    sample.append(hist[attr_col].tolist()[:i])\n            if i != len(pos_list) - 1:\n                if mode == 0:  #point-wise, the last col is label_col, include label 0 and 1\n                    last_col = \"label\"\n                    train_set.append(sample + [1]) #正样本\n                    for _ in range(neg_ratio):\n                        sample[1] = neg_list[neg_idx]\n                        neg_idx += 1\n                        train_set.append(sample + [0])\n                elif mode == 1:  #pair-wise, the last col is neg_col, include one negative item\n                    last_col = \"neg_items\"\n                    for _ in range(neg_ratio):\n                        sample_copy = copy.deepcopy(sample)\n                        sample_copy.append(neg_list[neg_idx])\n                        neg_idx += 1\n                        train_set.append(sample_copy)\n                elif mode == 2:  #list-wise, the last col is neg_col, include neg_ratio negative items\n                    last_col = \"neg_items\"\n                    sample.append(neg_list[neg_idx: neg_idx + neg_ratio])\n                    neg_idx += neg_ratio\n                    train_set.append(sample)\n                else:\n                    raise ValueError(\"mode should in (0,1,2)\")\n            else:\n                test_set.append(sample + [1])  #Note: if mode=1 or 2, the label col is useless.\n\n    random.shuffle(train_set)\n    random.shuffle(test_set)\n\n    print(\"n_train: %d, n_test: %d\" % (len(train_set), len(test_set)))\n    print(\"%d cold start user droped \" % (n_cold_user))\n\n    attr_hist_col = [\"hist_\" + col for col in item_attribute_cols]\n    df_train = pd.DataFrame(train_set,\n                            columns=[user_col, item_col, \"hist_\" + item_col, \"histlen_\" + item_col] + attr_hist_col + [last_col])\n    df_test = pd.DataFrame(test_set,\n                           columns=[user_col, item_col, \"hist_\" + item_col, \"histlen_\" + item_col] + attr_hist_col + [last_col])\n\n    return df_train, df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:53:59.847265Z","iopub.execute_input":"2025-09-14T08:53:59.847573Z","iopub.status.idle":"2025-09-14T08:53:59.863087Z","shell.execute_reply.started":"2025-09-14T08:53:59.847552Z","shell.execute_reply":"2025-09-14T08:53:59.862256Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def gen_model_input(df, user_profile, user_col, item_profile, item_col, seq_max_len, padding='pre', truncating='pre'):\n    # merge user_profile and item_profile, pad history seuence feature\n    df = pd.merge(df, user_profile, on=user_col, how='left')  # how=left to keep samples order same as the input\n    df = pd.merge(df, item_profile, on=item_col, how='left')\n    for col in df.columns.to_list():\n        if col.startswith(\"hist_\"):\n            df[col] = pad_sequences(df[col], maxlen=seq_max_len, value=0, padding=padding, truncating=truncating).tolist()\n    input_dict = df_to_dict(df)\n    return input_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:54:02.465850Z","iopub.execute_input":"2025-09-14T08:54:02.466147Z","iopub.status.idle":"2025-09-14T08:54:02.472891Z","shell.execute_reply.started":"2025-09-14T08:54:02.466124Z","shell.execute_reply":"2025-09-14T08:54:02.471768Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"! pip install torch_rechub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:34:26.256624Z","iopub.execute_input":"2025-09-14T08:34:26.256965Z","iopub.status.idle":"2025-09-14T08:35:57.311442Z","shell.execute_reply.started":"2025-09-14T08:34:26.256941Z","shell.execute_reply":"2025-09-14T08:35:57.310179Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting torch_rechub\n  Downloading torch_rechub-0.0.3-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (1.26.4)\nRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (2.2.3)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (1.2.2)\nRequirement already satisfied: annoy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from torch_rechub) (1.17.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->torch_rechub) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->torch_rechub) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->torch_rechub) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->torch_rechub) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.2->torch_rechub) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.2->torch_rechub) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.2->torch_rechub) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.0->torch_rechub)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torch_rechub) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->torch_rechub) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->torch_rechub) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->torch_rechub) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->torch_rechub) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->torch_rechub) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->torch_rechub) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->torch_rechub) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->torch_rechub) (2024.2.0)\nDownloading torch_rechub-0.0.3-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_rechub\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_rechub-0.0.3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"! pip install pymilvus","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:37:10.260904Z","iopub.execute_input":"2025-09-14T08:37:10.261374Z","iopub.status.idle":"2025-09-14T08:37:41.116932Z","shell.execute_reply.started":"2025-09-14T08:37:10.261344Z","shell.execute_reply":"2025-09-14T08:37:41.115697Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting pymilvus\n  Downloading pymilvus-2.6.1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.2.0)\nRequirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.73.1)\nCollecting protobuf>=5.27.2 (from pymilvus)\n  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nCollecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.10.0)\nRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.2.3)\nCollecting milvus-lite>=2.4.0 (from pymilvus)\n  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=1.2.4->pymilvus) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=1.2.4->pymilvus) (2024.2.0)\nDownloading pymilvus-2.6.1-py3-none-any.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.3/254.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, protobuf, milvus-lite, pymilvus\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed milvus-lite-2.5.1 protobuf-6.32.1 pymilvus-2.6.1 python-dotenv-1.1.1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch_rechub.utils.match import generate_seq_feature_match, gen_model_input\ndf_train, df_test = generate_seq_feature_match(data,user_col,item_col,time_col=\"Timestamp\",item_attribute_cols=[],sample_method=1, mode=0,neg_ratio=3,min_item=0) # 该函数将在 1.5 中讲解\nprint(df_train.head())\nprint(df_test.head())\n\nx_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=50)  # 该函数将在 1.5 中讲解\ny_train = x_train[\"label\"]\nx_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=50)\ny_test = x_test[\"label\"]\ndel x_train[\"label\"]  # 删除 y 值\ndel x_test[\"label\"]\n\nprint({k: v[:3] for k, v in x_train.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:54:06.032379Z","iopub.execute_input":"2025-09-14T08:54:06.032802Z","iopub.status.idle":"2025-09-14T08:56:44.568867Z","shell.execute_reply.started":"2025-09-14T08:54:06.032772Z","shell.execute_reply":"2025-09-14T08:56:44.567344Z"}},"outputs":[{"name":"stdout","text":"preprocess data\n","output_type":"stream"},{"name":"stderr","text":"generate sequence features: 100%|██████████| 6040/6040 [00:31<00:00, 190.42it/s] \n","output_type":"stream"},{"name":"stdout","text":"n_train: 3952516, n_test: 6040\n0 cold start user dropped \n   UserID  MovieID                                       hist_MovieID  \\\n0    3621      297  [2001, 108, 2985, 860, 361, 1121, 1782, 2965, ...   \n1    5592     1540  [1274, 1483, 577, 1513, 2087, 1487, 3342, 3221...   \n2    5519     2006  [2427, 1019, 1132, 3460, 2462, 2372, 714, 528,...   \n3     637     1421  [848, 1278, 2588, 1905, 864, 1114, 955, 905, 3...   \n4    1899     1442  [2657, 1107, 3176, 2415, 2856, 1140, 1260, 855...   \n\n   histlen_MovieID  label  \n0              201      0  \n1               55      1  \n2              240      0  \n3              314      0  \n4              659      1  \n   UserID  MovieID                                       hist_MovieID  \\\n0    2202     3074  [1635, 1004, 2363, 1114, 3578, 3652, 3702, 102...   \n1       8     3034  [1121, 108, 848, 3249, 467, 1446, 576, 384, 31...   \n2      40     2612  [860, 2330, 511, 1121, 3024, 909, 859, 1171, 1...   \n3      50     3616  [2146, 23, 842, 1828, 511, 3160, 3323, 1470, 3...   \n4    2786     1634  [108, 844, 2513, 2070, 1179, 32, 848, 843, 117...   \n\n   histlen_MovieID  label  \n0               70      1  \n1              138      1  \n2               95      1  \n3               42      1  \n4              297      1  \n{'UserID': array([3621, 5592, 5519]), 'MovieID': array([ 297, 1540, 2006]), 'hist_MovieID': array([[2275, 1486, 3156, 1509, 3244, 1355, 1459, 1585, 1600,  698,  628,\n        1730, 1021, 1467, 1567,  504,  438, 2692, 1508, 3039,  859, 2206,\n        1091, 1033, 1840, 1147,  848, 2977, 1765, 1196, 1014,  948, 2732,\n        2170, 3547, 1773, 2524, 1770, 1185, 2848, 1952, 1156, 2953,  949,\n        3272, 1132, 1105,  803, 2920, 1181],\n       [1487, 3342, 3221, 3322,  528, 1107,  254, 1111, 2375, 3239, 1179,\n        1121, 1210,  650, 2959, 1149, 1276,  576, 1110,   32, 3463, 2758,\n         467, 3460, 2171, 3458, 1026, 3295, 1278,  596, 1730, 3459, 1051,\n        1053, 2439, 1936, 1259, 2262, 1842, 1960, 1407, 1446, 2333, 1450,\n        1454, 1993, 1925, 2856, 1277, 2215],\n       [2622, 3034,  633, 1460, 1334,  692, 1136, 1213, 2476, 1156, 1118,\n         853, 1135, 1212, 2977,  843,  852,   41,  568,  376,  848, 1205,\n         108, 1151, 1765,  893, 2651, 1181, 2512, 1773, 1191, 1775, 2982,\n        2657, 2979, 3315, 1414, 3437, 3108, 3187, 2839, 2959, 1119, 3313,\n         549, 1768,  992, 1178, 2976, 1783]]), 'histlen_MovieID': array([201,  55, 240]), 'Gender': array([2, 2, 2]), 'Age': array([2, 3, 4]), 'Occupation': array([ 5,  2, 16]), 'Zip-code': array([1182, 1793, 1198]), 'Genre': array([177,  37, 274])}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"print(data.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:40:14.684942Z","iopub.execute_input":"2025-09-14T08:40:14.685377Z","iopub.status.idle":"2025-09-14T08:40:14.693389Z","shell.execute_reply.started":"2025-09-14T08:40:14.685335Z","shell.execute_reply":"2025-09-14T08:40:14.692321Z"}},"outputs":[{"name":"stdout","text":"Index(['UserID', 'MovieID', 'Rating', 'Timestamp', 'Gender', 'Age',\n       'Occupation', 'Zip-code', 'Title', 'Genre'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(df_train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:46:31.300024Z","iopub.execute_input":"2025-09-14T08:46:31.300415Z","iopub.status.idle":"2025-09-14T08:46:31.306884Z","shell.execute_reply.started":"2025-09-14T08:46:31.300388Z","shell.execute_reply":"2025-09-14T08:46:31.305782Z"}},"outputs":[{"name":"stdout","text":"Index(['UserID', 'UserID', 'hist_UserID', 'histlen_UserID', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from torch_rechub.basic.features import SparseFeature, SequenceFeature\n\n# embed_dim 是指定 LabelEncoder 的维度，会通过训练来自动学习到合适的 Lookup table\nuser_features = [\n    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in user_cols\n]\nuser_features += [\n    SequenceFeature(\"hist_MovieID\", vocab_size=feature_max_idx[\"MovieID\"], embed_dim=16, pooling=\"mean\", shared_with=\"MovieID\") # mean pooling，会对历史观影的 embedding 做平均运算\n]\n\nitem_features = [\n    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in item_cols\n]\n\nprint(user_features[1].name)\nprint(user_features[1].get_embedding_layer())\nprint(user_features[1].get_embedding_layer()._parameters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:00:56.654858Z","iopub.execute_input":"2025-09-14T09:00:56.655327Z","iopub.status.idle":"2025-09-14T09:00:56.783510Z","shell.execute_reply.started":"2025-09-14T09:00:56.655293Z","shell.execute_reply":"2025-09-14T09:00:56.782418Z"}},"outputs":[{"name":"stdout","text":"Gender\nEmbedding(3, 16)\n{'weight': Parameter containing:\ntensor([[-2.4859e-04,  5.0269e-06, -7.7867e-05,  1.7075e-04, -2.7757e-05,\n          1.0075e-04, -9.0587e-05,  1.0367e-04, -8.2237e-05,  1.8602e-05,\n         -2.9225e-05, -4.2575e-05, -5.1328e-05,  8.4033e-06,  7.4663e-05,\n          1.2297e-04],\n        [-2.0307e-05,  1.0396e-04, -4.5262e-05,  7.2058e-05, -1.2348e-04,\n         -7.7943e-05,  8.5386e-05, -1.1952e-04,  9.4706e-05, -3.4794e-05,\n         -3.1032e-05,  1.0975e-04, -2.7145e-05, -1.4672e-04,  3.7197e-05,\n         -2.6488e-05],\n        [ 1.3902e-04, -1.2960e-04, -1.7968e-05,  5.4971e-05, -5.2731e-05,\n         -5.4153e-05,  1.2404e-04, -6.7036e-05,  1.3234e-04,  4.0145e-05,\n         -3.7024e-05,  8.2626e-05, -7.0833e-05, -3.6584e-05, -6.9400e-05,\n          1.2227e-04]], requires_grad=True)}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# 将dataframe转为dict\nfrom torch_rechub.utils.data import df_to_dict\nall_item = df_to_dict(item_profile)\ntest_user = x_test\nprint({k: v[:3] for k, v in all_item.items()})\nprint({k: v[0] for k, v in test_user.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:01:09.974375Z","iopub.execute_input":"2025-09-14T09:01:09.974875Z","iopub.status.idle":"2025-09-14T09:01:09.988576Z","shell.execute_reply.started":"2025-09-14T09:01:09.974839Z","shell.execute_reply":"2025-09-14T09:01:09.987629Z"}},"outputs":[{"name":"stdout","text":"{'MovieID': array([1105,  640,  854]), 'Genre': array([240, 153, 283])}\n{'UserID': 2202, 'MovieID': 3074, 'hist_MovieID': array([1873,  252,  310, 2204, 1694, 1174, 2558, 2308,   17,    1, 2496,\n         34, 2203,  347, 3030,   11,   39, 2129,  428, 1399,  978, 2405,\n        575,  137,  102,  743, 2376,  225, 2555,  363, 1716, 2470,  526,\n       1458, 2502,  211, 1594,  330, 3024, 2385,  573,  800,  642, 1730,\n       1945, 3178, 3280, 3556, 3076, 3006]), 'histlen_MovieID': 70, 'Gender': 1, 'Age': 1, 'Occupation': 11, 'Zip-code': 504, 'Genre': 186}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import torch\nclass DSSM(torch.nn.Module):\n    \"\"\"Deep Structured Semantic Model\n\n    Args:\n        user_features (list[Feature Class]): training by the user tower module.\n        item_features (list[Feature Class]): training by the item tower module.\n        temperature (float): temperature factor for similarity score, default to 1.0.\n        user_params (dict): the params of the User Tower module, keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}.\n        item_params (dict): the params of the Item Tower module, keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}.\n    \"\"\"\n\n    def __init__(self, user_features, item_features, user_params, item_params, temperature=1.0):\n        super().__init__()\n        self.user_features = user_features\n        self.item_features = item_features\n        self.temperature = temperature\n        self.user_dims = sum([fea.embed_dim for fea in user_features])\n        self.item_dims = sum([fea.embed_dim for fea in item_features])\n\n        self.embedding = EmbeddingLayer(user_features + item_features)\n        self.user_mlp = MLP(self.user_dims, output_layer=False, **user_params)\n        self.item_mlp = MLP(self.item_dims, output_layer=False, **item_params)\n        self.mode = None\n\n    def forward(self, x):\n        user_embedding = self.user_tower(x)\n        item_embedding = self.item_tower(x)\n        if self.mode == \"user\":\n            return user_embedding\n        if self.mode == \"item\":\n            return item_embedding\n\n        # calculate cosine score\n        y = torch.mul(user_embedding, item_embedding).sum(dim=1)\n        # y = y / self.temperature\n        return torch.sigmoid(y)\n\n    def user_tower(self, x):\n        if self.mode == \"item\":\n            return None\n        input_user = self.embedding(x, self.user_features, squeeze_dim=True)  #[batch_size, num_features*deep_dims]\n        user_embedding = self.user_mlp(input_user)  #[batch_size, user_params[\"dims\"][-1]]\n        user_embedding = F.normalize(user_embedding, p=2, dim=1)  # L2 normalize\n        return user_embedding\n\n    def item_tower(self, x):\n        if self.mode == \"user\":\n            return None\n        input_item = self.embedding(x, self.item_features, squeeze_dim=True)  #[batch_size, num_features*embed_dim]\n        item_embedding = self.item_mlp(input_item)  #[batch_size, item_params[\"dims\"][-1]]\n        item_embedding = F.normalize(item_embedding, p=2, dim=1)\n        return item_embedding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:01:40.636223Z","iopub.execute_input":"2025-09-14T09:01:40.636527Z","iopub.status.idle":"2025-09-14T09:01:40.649009Z","shell.execute_reply.started":"2025-09-14T09:01:40.636508Z","shell.execute_reply":"2025-09-14T09:01:40.647848Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from torch_rechub.models.matching import DSSM\nfrom torch_rechub.trainers import MatchTrainer\nfrom torch_rechub.utils.data import MatchDataGenerator\n\n# 根据之前处理的数据拿到Dataloader\ndg = MatchDataGenerator(x=x_train, y=y_train)\ntrain_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=256)\n\n# 定义模型\nmodel = DSSM(user_features, item_features, temperature=0.02,  # 在归一化之后的向量计算內积之后，乘一个固定的超参 r ，论文中命名为温度系数。归一化后如果不乘 temperature，模型无法收敛\n             user_params={\n                 \"dims\": [256, 128, 64],\n                 \"activation\": 'prelu',  # important!!\n             },\n             item_params={\n                 \"dims\": [256, 128, 64],\n                 \"activation\": 'prelu',  # important!!\n             })\n\n# 模型训练器\ntrainer = MatchTrainer(model,\n                       mode=0,  # 同上面的mode，需保持一致\n                       optimizer_params={\n                           \"lr\": 1e-4,\n                           \"weight_decay\": 1e-6\n                       },\n                       n_epoch=10,\n                       device='cpu',\n                       model_path=save_dir)\n\n# 开始训练\ntrainer.fit(train_dl)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:01:58.290786Z","iopub.execute_input":"2025-09-14T09:01:58.291134Z","iopub.status.idle":"2025-09-14T10:19:28.285066Z","shell.execute_reply.started":"2025-09-14T09:01:58.291083Z","shell.execute_reply":"2025-09-14T10:19:28.283564Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"train:   0%|          | 0/15440 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\ntrain: 100%|██████████| 15440/15440 [07:18<00:00, 35.19it/s, loss=0.551]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [07:23<00:00, 34.80it/s, loss=0.566]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [07:16<00:00, 35.36it/s, loss=0.562]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [07:23<00:00, 34.83it/s, loss=0.548]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [07:28<00:00, 34.46it/s, loss=0.549]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 5\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [08:13<00:00, 31.31it/s, loss=0.551]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 6\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [08:11<00:00, 31.39it/s, loss=0.551]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 7\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [07:55<00:00, 32.44it/s, loss=0.552]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 8\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [08:09<00:00, 31.56it/s, loss=0.55] \n","output_type":"stream"},{"name":"stdout","text":"epoch: 9\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 15440/15440 [08:05<00:00, 31.83it/s, loss=0.546]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nfrom torch_rechub.utils.match import Annoy\nfrom torch_rechub.basic.metric import topk_metrics\n\ndef match_evaluation(user_embedding, item_embedding, test_user, all_item, user_col='UserID', item_col='MovieID',\n                     raw_id_maps=\"./raw_id_maps.npy\", topk=10):\n    print(\"evaluate embedding matching on test data\")\n    annoy = Annoy(n_trees=10)\n    annoy.fit(item_embedding)\n\n    #for each user of test dataset, get ann search topk result\n    print(\"matching for topk\")\n    user_map, item_map = np.load(raw_id_maps, allow_pickle=True)\n    match_res = collections.defaultdict(dict)  # user id -> predicted item ids\n    for user_id, user_emb in zip(test_user[user_col], user_embedding):\n        items_idx, items_scores = annoy.query(v=user_emb, n=topk)  #the index of topk match items\n        match_res[user_map[user_id]] = np.vectorize(item_map.get)(all_item[item_col][items_idx])\n\n    #get ground truth\n    print(\"generate ground truth\")\n\n    data = pd.DataFrame({user_col: test_user[user_col], item_col: test_user[item_col]})\n    data[user_col] = data[user_col].map(user_map)\n    data[item_col] = data[item_col].map(item_map)\n    user_pos_item = data.groupby(user_col).agg(list).reset_index()\n    ground_truth = dict(zip(user_pos_item[user_col], user_pos_item[item_col]))  # user id -> ground truth\n\n    print(\"compute topk metrics\")\n    out = topk_metrics(y_true=ground_truth, y_pred=match_res, topKs=[topk])\n    return out\n\nuser_embedding = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=save_dir)\nitem_embedding = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=save_dir)\n\nmatch_evaluation(user_embedding, item_embedding, test_user, all_item, topk=10, raw_id_maps=save_dir+\"raw_id_maps.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T10:19:28.287823Z","iopub.execute_input":"2025-09-14T10:19:28.288948Z","iopub.status.idle":"2025-09-14T10:19:31.292036Z","shell.execute_reply.started":"2025-09-14T10:19:28.288911Z","shell.execute_reply":"2025-09-14T10:19:31.290918Z"}},"outputs":[{"name":"stderr","text":"user inference: 100%|██████████| 24/24 [00:01<00:00, 19.45it/s]\nitem inference: 100%|██████████| 15/15 [00:01<00:00, 14.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluate embedding matching on test data\nmatching for topk\ngenerate ground truth\ncompute topk metrics\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"defaultdict(list,\n            {'NDCG': ['NDCG@10: 0.0144'],\n             'MRR': ['MRR@10: 0.0101'],\n             'Recall': ['Recall@10: 0.029'],\n             'Hit': ['Hit@10: 0.029'],\n             'Precision': ['Precision@10: 0.0029']})"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}